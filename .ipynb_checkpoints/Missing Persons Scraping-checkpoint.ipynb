{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13eb7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#from bs4 import soupStrainer\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathp60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=Pre-1960s'\n",
    "path60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1960s'\n",
    "path70s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1970s'\n",
    "path80s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1980s'\n",
    "path90s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1990s'\n",
    "path00s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2000s'\n",
    "path10s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2010s'\n",
    "path20s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2020s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78135583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "##results = soup.find_all('div', attrs={'class':'cases'})\n",
    "\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href') \n",
    "        if href and \"https://charleyproject.org/case/\" in href: \n",
    "            links.append(href)\n",
    "    \n",
    "#if contains 'https://charleyproject.org/case/ return to new df.\n",
    "##pre_1960s = pd.DataFrame(pre_1960s)\n",
    "    df = pd.DataFrame(links, columns=['Links:']) \n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac0f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1960s = extract_links(pathp60s)\n",
    "x_1960s = extract_links(path60s)\n",
    "x_1970s = extract_links(path70s)\n",
    "x_1980s = extract_links(path80s)\n",
    "x_1990s = extract_links(path90s)\n",
    "x_2000s = extract_links(path00s)\n",
    "x_2010s = extract_links(path10s)\n",
    "x_2020s = extract_links(path20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d34c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data1 = pd.concat([pre_1960s, x_1960s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b020c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data2 = pd.concat([x_1970s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9063890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data3 = pd.concat([x_1980s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41041904",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data4 = pd.concat([x_1990s, x_2000s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20001a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data5 = pd.concat([x_2010s, x_2020s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3493d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 330/330 [04:08<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame\n",
    "missing_people_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data1.shape[0])):\n",
    "    link_str = years_data1.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    # Start building person dict\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    # Fill in details\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skip if value missing or malformed\n",
    "\n",
    "    # Convert once per person, not per tag\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cb8e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [12:36<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "missing_people_df1 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data2.shape[0])):\n",
    "    link_str = years_data2.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    # Now convert only once per person\n",
    "    persondf1 = pd.DataFrame(person)\n",
    "    missing_people_df1 = pd.concat([missing_people_df1, persondf1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af7c601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1976/1976 [25:28<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Links:                  name  \\\n",
      "0     https://charleyproject.org/case/tammy-sue-clem...    Tammy Sue Clements   \n",
      "1     https://charleyproject.org/case/kelle-renee-fe...  Kelle Renee Ferstrom   \n",
      "2     https://charleyproject.org/case/kimberly-ann-k...   Kimberly Ann Kahler   \n",
      "3      https://charleyproject.org/case/alma-violet-root      Alma Violet Root   \n",
      "4     https://charleyproject.org/case/donald-allen-w...   Donald Allen Warren   \n",
      "...                                                 ...                   ...   \n",
      "1971  https://charleyproject.org/case/judith-fern-ga...    Judith Fern Garcia   \n",
      "1972   https://charleyproject.org/case/robin-lynn-ricci      Robin Lynn Ricci   \n",
      "1973     https://charleyproject.org/case/ronald-l-janes       Ronald L. Janes   \n",
      "1974  https://charleyproject.org/case/karen-anne-spe...    Karen Anne Spencer   \n",
      "1975   https://charleyproject.org/case/nerissa-franklin      Nerissa Franklin   \n",
      "\n",
      "     Missing Since                                Missing From  \\\n",
      "0       01/01/1980           Lincoln, \\t\\t\\t\\t\\t\\t\\t\\tNebraska   \n",
      "1       01/01/1980              Houston, \\t\\t\\t\\t\\t\\t\\t\\tTexas   \n",
      "2       01/01/1980  Marina Del Rey, \\t\\t\\t\\t\\t\\t\\t\\tCalifornia   \n",
      "3       01/01/1980          Auburn, \\t\\t\\t\\t\\t\\t\\t\\tCalifornia   \n",
      "4       01/01/1980             Pahrump, \\t\\t\\t\\t\\t\\t\\t\\tNevada   \n",
      "...            ...                                         ...   \n",
      "1971    12/24/1989          Laguna, \\t\\t\\t\\t\\t\\t\\t\\tNew Mexico   \n",
      "1972    12/25/1989    West Palm Beach, \\t\\t\\t\\t\\t\\t\\t\\tFlorida   \n",
      "1973    12/27/1989        Louisville, \\t\\t\\t\\t\\t\\t\\t\\tKentucky   \n",
      "1974    12/29/1989        Miami Township, \\t\\t\\t\\t\\t\\t\\t\\tOhio   \n",
      "1975    12/30/1989        Gautier, \\t\\t\\t\\t\\t\\t\\t\\tMississippi   \n",
      "\n",
      "          Classification     Sex             Race    Date of Birth  \\\n",
      "0     Endangered Missing  Female            White  05/09/1965 (60)   \n",
      "1                Missing  Female            White              NaN   \n",
      "2     Endangered Missing  Female            White  01/17/1964 (61)   \n",
      "3     Endangered Missing  Female            White  02/08/1965 (60)   \n",
      "4                Missing    Male            White              NaN   \n",
      "...                  ...     ...              ...              ...   \n",
      "1971             Missing  Female  Native American  01/08/1968 (57)   \n",
      "1972  Endangered Missing  Female            White  06/04/1964 (61)   \n",
      "1973             Missing    Male            White              NaN   \n",
      "1974  Endangered Missing  Female            White  01/17/1972 (53)   \n",
      "1975  Endangered Runaway  Female            White  02/28/1974 (51)   \n",
      "\n",
      "                    Age            Height and Weight  \\\n",
      "0     14 - 15 years old  5'2 - 5'5, 120 - 130 pounds   \n",
      "1          20 years old              5'6, 135 pounds   \n",
      "2          15 years old              5'8, 145 pounds   \n",
      "3          14 years old              5'4, 130 pounds   \n",
      "4          51 years old  5'4 - 5'5, 120 - 150 pounds   \n",
      "...                 ...                          ...   \n",
      "1971       21 years old              5'2, 138 pounds   \n",
      "1972       25 years old              5'4, 120 pounds   \n",
      "1973       33 years old             5'10, 160 pounds   \n",
      "1974       17 years old              5'1, 120 pounds   \n",
      "1975       15 years old              4'4, 130 pounds   \n",
      "\n",
      "                         Distinguishing Characteristics  \\\n",
      "0     Caucasian female. Blond/brown hair, brown eyes...   \n",
      "1     Caucasian female. Brown hair, brown eyes. Fers...   \n",
      "2     Caucasian female. Blonde hair, blue eyes. Kimb...   \n",
      "3     Caucasian female. Brown hair, brown eyes. Alma...   \n",
      "4     Caucasian male. Brown hair, hazel eyes. Warren...   \n",
      "...                                                 ...   \n",
      "1971  Native American female. Black hair, brown eyes...   \n",
      "1972  Caucasian female. Brown hair, brown eyes. Ricc...   \n",
      "1973  Caucasian male. Graying black hair, hazel eyes...   \n",
      "1974  Caucasian female. Blonde hair, hazel/green eye...   \n",
      "1975  Caucasian female. Brown hair, brown eyes. Neri...   \n",
      "\n",
      "                           Clothing/Jewelry Description  \\\n",
      "0                                                   NaN   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                                   NaN   \n",
      "...                                                 ...   \n",
      "1971  A Levi's jacket with faded brown or tan trim o...   \n",
      "1972                                                NaN   \n",
      "1973  A gray jacket and blue jeans. Carrying a blue ...   \n",
      "1974     A black jacket, a lavender top and blue jeans.   \n",
      "1975                                                NaN   \n",
      "\n",
      "                                     Medical Conditions  \\\n",
      "0                                                   NaN   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                                   NaN   \n",
      "...                                                 ...   \n",
      "1971                                                NaN   \n",
      "1972                 Ricci has a history of drug abuse.   \n",
      "1973                                                NaN   \n",
      "1974  According to Karen's father, she had an alcoho...   \n",
      "1975                                                NaN   \n",
      "\n",
      "                                  Associated Vehicle(s)  \n",
      "0                                                   NaN  \n",
      "1                                                   NaN  \n",
      "2                                                   NaN  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "1971                                                NaN  \n",
      "1972                                                NaN  \n",
      "1973                                                NaN  \n",
      "1974  Black Buick Regal (accounted for), Red 1982 Da...  \n",
      "1975                                                NaN  \n",
      "\n",
      "[1976 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missing_people_df2 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data3.shape[0])):\n",
    "    link_str = years_data3.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df2 = pd.concat([missing_people_df2, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b83919ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'years_data4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m missing_people_df3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[43myears_data4\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m      4\u001b[0m     link_str \u001b[38;5;241m=\u001b[39m years_data4\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinks:\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(link_str)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'years_data4' is not defined"
     ]
    }
   ],
   "source": [
    "missing_people_df3 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data4.shape[0])):\n",
    "    link_str = years_data4.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df3 = pd.concat([missing_people_df3, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df4 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data5.shape[0])):\n",
    "    link_str = years_data5.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df4 = pd.concat([missing_people_df4, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25cea61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_people_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mmissing_people_df\u001b[49m, missing_people_df1, missing_people_df2], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missing_people_df' is not defined"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([missing_people_df, missing_people_df1, missing_people_df2, missing_people_df3, missing_people_df4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('Missing_Persons_Updated_7-10-25.xlsx')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
