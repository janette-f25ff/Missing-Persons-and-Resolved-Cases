{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#from bs4 import soupStrainer\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathp60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=Pre-1960s'\n",
    "path60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1960s'\n",
    "path70s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1970s'\n",
    "path80s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1980s'\n",
    "path90s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1990s'\n",
    "path00s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2000s'\n",
    "path10s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2010s'\n",
    "path20s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2020s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78135583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "##results = soup.find_all('div', attrs={'class':'cases'})\n",
    "\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href') \n",
    "        if href and \"https://charleyproject.org/case/\" in href: \n",
    "            links.append(href)\n",
    "    \n",
    "#if contains 'https://charleyproject.org/case/ return to new df.\n",
    "##pre_1960s = pd.DataFrame(pre_1960s)\n",
    "    df = pd.DataFrame(links, columns=['Links:']) \n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac0f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1960s = extract_links(pathp60s)\n",
    "x_1960s = extract_links(path60s)\n",
    "x_1970s = extract_links(path70s)\n",
    "x_1980s = extract_links(path80s)\n",
    "x_1990s = extract_links(path90s)\n",
    "x_2000s = extract_links(path00s)\n",
    "x_2010s = extract_links(path10s)\n",
    "x_2020s = extract_links(path20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d34c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data1 = pd.concat([pre_1960s, x_1960s, x_1970s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41041904",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data2 = pd.concat([x_1980s, x_1990s, x_2000s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a5f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data3 = pd.concat([x_2010s, x_2020s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b00b7",
   "metadata": {},
   "source": [
    "# Initialize an empty DataFrame to store all data \n",
    "missing_people_df = pd.DataFrame() \n",
    "# Iterate over each link in all_years_data \n",
    "for i in tqdm(range(years_data1.shape[0])): \n",
    "    link_str = years_data1.at[i, 'Links:'] \n",
    "    r2 = requests.get(link_str) \n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser') \n",
    "    # Extract data \n",
    "    person = { \"Links:\": [link_str], \n",
    "              \"name\": [soup2.find('h1').get_text()] \n",
    "             } \n",
    "    for strong_tag in soup2.find_all('strong'): \n",
    "        k = strong_tag.text.strip() \n",
    "        v = strong_tag.next_sibling.strip() \n",
    "        person[k] = [v] \n",
    "        # Convert to DataFrame \n",
    "        persondf = pd.DataFrame(person) \n",
    "        # Merge with the main DataFrame \n",
    "        if not missing_people_df.empty: \n",
    "            missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True) \n",
    "        else: \n",
    "            missing_people_df = persondf \n",
    "# Display the final DataFrame \n",
    "print(missing_people_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f45ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▌                                                                  | 202/1272 [03:00<13:08,  1.36it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame\n",
    "missing_people_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data1.shape[0])):\n",
    "    link_str = years_data1.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    # Start building person dict\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    # Fill in details\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skip if value missing or malformed\n",
    "\n",
    "    # Convert once per person, not per tag\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d44442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_people_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ef96b",
   "metadata": {},
   "source": [
    "missing_people_df1 = pd.DataFrame() \n",
    "# Iterate over each link in all_years_data \n",
    "for i in tqdm(range(years_data2.shape[0])): \n",
    "    link_str = years_data2.at[i, 'Links:'] \n",
    "    r2 = requests.get(link_str) \n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser') \n",
    "    # Extract data \n",
    "    person = { \"Links:\": [link_str], \n",
    "              \"name\": [soup2.find('h1').get_text()] \n",
    "             } \n",
    "    for strong_tag in soup2.find_all('strong'): \n",
    "        k = strong_tag.text.strip() \n",
    "        v = strong_tag.next_sibling.strip() \n",
    "        person[k] = [v] \n",
    "        # Convert to DataFrame \n",
    "        persondf1 = pd.DataFrame(person) \n",
    "        # Merge with the main DataFrame \n",
    "        if not missing_people_df1.empty: \n",
    "            missing_people_df1 = pd.concat([missing_people_df1, persondf1], ignore_index=True) \n",
    "        else: \n",
    "            missing_people_df1 = persondf1\n",
    "# Display the final DataFrame \n",
    "print(missing_people_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5990e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df1 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data2.shape[0])):\n",
    "    link_str = years_data2.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    # Now convert only once per person\n",
    "    persondf1 = pd.DataFrame(person)\n",
    "    missing_people_df1 = pd.concat([missing_people_df1, persondf1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca88c04",
   "metadata": {},
   "source": [
    "# Initialize an empty DataFrame to store all data \n",
    "missing_people_df2 = pd.DataFrame() \n",
    "# Iterate over each link in all_years_data \n",
    "for i in tqdm(range(years_data3.shape[0])): \n",
    "    link_str = years_data3.at[i, 'Links:'] \n",
    "    r2 = requests.get(link_str) \n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser') \n",
    "    # Extract data \n",
    "    person = { \"Links:\": [link_str], \n",
    "              \"name\": [soup2.find('h1').get_text()] \n",
    "             } \n",
    "    for strong_tag in soup2.find_all('strong'): \n",
    "        k = strong_tag.text.strip() \n",
    "        v = strong_tag.next_sibling.strip() \n",
    "        person[k] = [v] \n",
    "        # Convert to DataFrame \n",
    "        persondf = pd.DataFrame(person) \n",
    "        # Merge with the main DataFrame \n",
    "        if not missing_people_df.empty: \n",
    "            missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True) \n",
    "        else: \n",
    "            missing_people_df = persondf \n",
    "# Display the final DataFrame \n",
    "print(missing_people_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df2 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data3.shape[0])):\n",
    "    link_str = years_data3.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df2 = pd.concat([missing_people_df2, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cea61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([missing_people_df, missing_people_df1, missing_people_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('Missing_Persons_Updated_7-9-25.xlsx')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
