{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#from bs4 import soupStrainer\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathp60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=Pre-1960s'\n",
    "path60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1960s'\n",
    "path70s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1970s'\n",
    "path80s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1980s'\n",
    "path90s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1990s'\n",
    "path00s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2000s'\n",
    "path10s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2010s'\n",
    "path20s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2020s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78135583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "##results = soup.find_all('div', attrs={'class':'cases'})\n",
    "\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href') \n",
    "        if href and \"https://charleyproject.org/case/\" in href: \n",
    "            links.append(href)\n",
    "    \n",
    "#if contains 'https://charleyproject.org/case/ return to new df.\n",
    "##pre_1960s = pd.DataFrame(pre_1960s)\n",
    "    df = pd.DataFrame(links, columns=['Links:']) \n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac0f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1960s = extract_links(pathp60s)\n",
    "x_1960s = extract_links(path60s)\n",
    "x_1970s = extract_links(path70s)\n",
    "x_1980s = extract_links(path80s)\n",
    "x_1990s = extract_links(path90s)\n",
    "x_2000s = extract_links(path00s)\n",
    "x_2010s = extract_links(path10s)\n",
    "x_2020s = extract_links(path20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d34c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data1 = pd.concat([pre_1960s, x_1960s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d57ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data2 = pd.concat([x_1970s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11261bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data3 = pd.concat([x_1980s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41041904",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data4 = pd.concat([x_1990s, x_2000s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_data5 = pd.concat([x_2010s, x_2020s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da3a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 330/330 [04:08<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame\n",
    "missing_people_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data1.shape[0])):\n",
    "    link_str = years_data1.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    # Start building person dict\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    # Fill in details\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skip if value missing or malformed\n",
    "\n",
    "    # Convert once per person, not per tag\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27600a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [12:36<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "missing_people_df1 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data2.shape[0])):\n",
    "    link_str = years_data2.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    # Now convert only once per person\n",
    "    persondf1 = pd.DataFrame(person)\n",
    "    missing_people_df1 = pd.concat([missing_people_df1, persondf1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885961e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                              | 31/1976 [00:27<26:11,  1.24it/s]"
     ]
    }
   ],
   "source": [
    "missing_people_df2 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data3.shape[0])):\n",
    "    link_str = years_data3.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df2 = pd.concat([missing_people_df2, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2da25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'years_data4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m missing_people_df3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[43myears_data4\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m      4\u001b[0m     link_str \u001b[38;5;241m=\u001b[39m years_data4\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinks:\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(link_str)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'years_data4' is not defined"
     ]
    }
   ],
   "source": [
    "missing_people_df3 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data4.shape[0])):\n",
    "    link_str = years_data4.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df3 = pd.concat([missing_people_df3, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df4 = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(years_data5.shape[0])):\n",
    "    link_str = years_data5.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        try:\n",
    "            k = strong_tag.text.strip()\n",
    "            v = strong_tag.next_sibling.strip()\n",
    "            person[k] = [v]\n",
    "        except AttributeError:\n",
    "            continue  # Skips malformed or missing values\n",
    "\n",
    "    # Only convert and append once\n",
    "    persondf = pd.DataFrame(person)\n",
    "    missing_people_df4 = pd.concat([missing_people_df4, persondf], ignore_index=True)\n",
    "\n",
    "print(missing_people_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cea61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([missing_people_df, missing_people_df1, missing_people_df2,missing_people_df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('Missing_Persons_Updated_7-9-25.xlsx')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
